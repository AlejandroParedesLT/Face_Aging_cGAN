{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Aging Using Conditional GAN with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on paper [\"Face Aging With Conditional Generative Adversarial Networks\"](https://arxiv.org/abs/1702.01983)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NKx9Ay3m2Odq"
   },
   "source": [
    "# Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wOPFp8-_k8c"
   },
   "outputs": [],
   "source": [
    "# Download faces only\n",
    "#!wget https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/wiki_crop.tar\n",
    "#!tar -xf wiki_crop.tar\n",
    "\n",
    "curl -O https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/wiki_crop.tar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qeU_6DdsuvKa"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from torchvision import  transforms#, datasets , utils\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OXdydrBGGDRH"
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KYEcMKo1TpUh"
   },
   "outputs": [],
   "source": [
    "def calc_age(taken, dob):\n",
    "    '''\n",
    "    Calculate age\n",
    "    :param taken: Date when photo taken\n",
    "    :param dob: Date of birth in serials\n",
    "    :return: age in years\n",
    "    '''\n",
    "    birth = datetime.fromordinal(max(int(dob) - 366, 1))\n",
    "\n",
    "    # assume the photo was taken in the middle of the year\n",
    "    if birth.month < 7:\n",
    "        return taken - birth.year\n",
    "    else:\n",
    "        return taken - birth.year - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2NEp6vcEZjhS"
   },
   "outputs": [],
   "source": [
    "def load_data(dataset='wiki', data_dir='./wiki_crop'):\n",
    "    '''\n",
    "    Load meta data and calculate age\n",
    "    :param dataset: dataset name, defaults = 'wiki'\n",
    "    :param data_dir: data directory, defaults = './wiki_crop'\n",
    "    :return: list of full_path and age\n",
    "    '''\n",
    "    # Load meta data\n",
    "    meta_path = Path(data_dir) / f'{dataset}.mat'\n",
    "    meta = loadmat(meta_path)\n",
    "    meta_data = meta[dataset][0, 0]\n",
    "\n",
    "    # Load all file paths\n",
    "    full_path = meta_data['full_path'][0]\n",
    "    full_path = [y for x in full_path for y in x]\n",
    "\n",
    "    # Load dates of birth\n",
    "    dob = meta_data['dob'][0]\n",
    "\n",
    "    # Load years when photo taken\n",
    "    photo_taken = meta_data['photo_taken'][0]\n",
    "\n",
    "    # Calculate age\n",
    "    age = [calc_age(photo_taken[i], dob[i]) for i in range(len(dob))]\n",
    "    \n",
    "    # Clean mapping with age > 0\n",
    "    clean_mapping = {pth:age for (pth, age) in zip(full_path, age) if age > 0}\n",
    "    \n",
    "    # List of full_path, age\n",
    "    full_path = list(clean_mapping.keys())\n",
    "    age = list(clean_mapping.values())\n",
    "\n",
    "    return full_path, age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wZ4JPgJTSr7h"
   },
   "outputs": [],
   "source": [
    "# helper scale function\n",
    "def scale(x, feature_range=(-1, 1)):\n",
    "    ''' Scale takes in an image x and returns that image, scaled\n",
    "       with a feature_range of pixel values from -1 to 1. \n",
    "       This function assumes that the input x is already scaled from 0-1.'''\n",
    "    # assume x is scaled to (0, 1)\n",
    "    # scale to feature_range and return scaled x\n",
    "    min, max = feature_range\n",
    "    x = x * (max - min) + min\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tCOceOxSZ4Vr"
   },
   "outputs": [],
   "source": [
    "# helper one-hot function\n",
    "bins = [18, 29, 39, 49, 59]\n",
    "def one_hot(x, bins):\n",
    "    '''\n",
    "    Convert tensor x to one-hot tensor\n",
    "    '''\n",
    "    x = x.numpy()\n",
    "    idxs = np.digitize(x, bins, right=True)\n",
    "    idxs = idxs.reshape(-1,1)\n",
    "    z = torch.zeros(len(x), len(bins)+1).scatter_(1, torch.tensor(idxs), 1)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AA3GehgpZ8U1"
   },
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q_l6WZxaZ5h3"
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize as rs, ToTensor as tt\n",
    "\n",
    "class ImageAgeDataset(Dataset):\n",
    "    '''Image and corresponding age Dataset'''\n",
    "    def __init__(self, dataset, data_dir, transform=None):\n",
    "        '''\n",
    "        :param dataset: Dataset name.\n",
    "        :param data_dir: Directory with all the images.\n",
    "        :param transform: Optional transform to be applied on sample\n",
    "        '''\n",
    "        self.data_dir = data_dir\n",
    "        self.full_path, self.age = load_data(dataset, data_dir)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.age)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(os.path.join(self.data_dir, self.full_path[idx]))\n",
    "        age = self.age[idx]\n",
    "        sample = {'image': image, 'age': age}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fcCFgFSaByO"
   },
   "outputs": [],
   "source": [
    "class Resize(object):\n",
    "    '''Resize the input PIL Image to the given size.'''\n",
    "    \n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        image, age = sample['image'], sample['age']\n",
    "        image = rs(self.output_size)(image)\n",
    "        return {'image': image, 'age': age}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, age = sample['image'], sample['age']\n",
    "        image = tt()(image)\n",
    "        # expand dept from 1 to 3 channels for gray images\n",
    "        if image.size()[0] == 1:\n",
    "            image = image.expand(3,-1,-1)\n",
    "\n",
    "        return {'image': image, 'age': age}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nxZr94rCDj1g"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Reader needs file name or open file-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ALEJANDRO\\Documents\\5. Programming\\.virtualenvs\\aplt_duke\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:39\u001b[0m, in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wiki\\\\wiki.mat'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tt(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Convert to tensor\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sample\n\u001b[1;32m---> 14\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageAgeDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwiki\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./wiki\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# build DataLoaders\u001b[39;00m\n\u001b[0;32m     17\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m=\u001b[39mtrain_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m, in \u001b[0;36mImageAgeDataset.__init__\u001b[1;34m(self, dataset, data_dir, transform)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m:param dataset: Dataset name.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m:param data_dir: Directory with all the images.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m:param transform: Optional transform to be applied on sample\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;241m=\u001b[39m data_dir\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mage \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(dataset, data_dir)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load meta data\u001b[39;00m\n\u001b[0;32m      9\u001b[0m meta_path \u001b[38;5;241m=\u001b[39m Path(data_dir) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 10\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m meta_data \u001b[38;5;241m=\u001b[39m meta[dataset][\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Load all file paths\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ALEJANDRO\\Documents\\5. Programming\\.virtualenvs\\aplt_duke\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:225\u001b[0m, in \u001b[0;36mloadmat\u001b[1;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03mLoad MATLAB file.\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m variable_names \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariable_names\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 225\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    226\u001b[0m     MR, _ \u001b[38;5;241m=\u001b[39m mat_reader_factory(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    227\u001b[0m     matfile_dict \u001b[38;5;241m=\u001b[39m MR\u001b[38;5;241m.\u001b[39mget_variables(variable_names)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ALEJANDRO\\Documents\\5. Programming\\.virtualenvs\\aplt_duke\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:17\u001b[0m, in \u001b[0;36m_open_file_context\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_context\u001b[39m(file_like, appendmat, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     f, opened \u001b[38;5;241m=\u001b[39m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "File \u001b[1;32mc:\\Users\\ALEJANDRO\\Documents\\5. Programming\\.virtualenvs\\aplt_duke\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:47\u001b[0m, in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_like, mode), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReader needs file name or open file-like object\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     49\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: Reader needs file name or open file-like object"
     ]
    }
   ],
   "source": [
    "dataset='wiki'\n",
    "data_dir=r\"D:\\temporal_CV\\1. AIPI 590 - Computer Vision\\assignment\\3. third_project\\wiki_preprocess\\wiki\"\n",
    "\n",
    "bins = [18, 29, 39, 49, 59]\n",
    "img_size = 64\n",
    "batch_size = 128\n",
    "#num_workers = 0\n",
    "\n",
    "def transform(sample):\n",
    "    sample['image'] = rs(sample['image'])  # Apply resizing\n",
    "    sample['image'] = tt(sample['image'])  # Convert to tensor\n",
    "    return sample\n",
    "\n",
    "train_dataset = ImageAgeDataset(dataset='wiki', data_dir='./wiki', transform=transform)\n",
    "\n",
    "# build DataLoaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2678
    },
    "colab_type": "code",
    "id": "eYLNycoYd8ON",
    "outputId": "e9547802-5640-4d6e-a94a-09de91e54ef9"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import Resize, ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "class ImageAgeDataset(Dataset):\n",
    "    '''Image and corresponding age Dataset'''\n",
    "    def __init__(self, dataset, data_dir, transform=None):\n",
    "        '''\n",
    "        :param dataset: Dataset name.\n",
    "        :param data_dir: Directory with all the images.\n",
    "        :param transform: Optional transform to be applied on sample\n",
    "        '''\n",
    "        self.data_dir = data_dir\n",
    "        self.full_path, self.age = load_data(dataset, data_dir)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.age)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(os.path.join(self.data_dir, self.full_path[idx]))\n",
    "        \n",
    "        # Convert the image to RGB (in case it's grayscale)\n",
    "        image = image.convert('RGB')\n",
    "        \n",
    "        age = self.age[idx]\n",
    "        \n",
    "        sample = {'image': image, 'age': age}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)  # Apply transform to the image and age\n",
    "        \n",
    "        return sample\n",
    "\n",
    "# Define your transformations outside of the dataset class\n",
    "img_size = 64\n",
    "resize = Resize((img_size, img_size))\n",
    "to_tensor = ToTensor()\n",
    "\n",
    "# Create a custom transform function\n",
    "def transform(sample):\n",
    "    sample['image'] = resize(sample['image'])  # Apply resizing\n",
    "    sample['image'] = to_tensor(sample['image'])  # Convert to tensor\n",
    "    return sample\n",
    "\n",
    "# Initialize your dataset with the custom transformation\n",
    "train_dataset = ImageAgeDataset(dataset='wiki', data_dir='./wiki', transform=transform)\n",
    "\n",
    "# Build DataLoader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128)\n",
    "\n",
    "# Fetch one batch\n",
    "data = next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "pblP2iGPeCnq",
    "outputId": "98efa8b9-4d81-4d6c-9ff2-070ef5491510"
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_dataset[0]['image'].numpy().transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "5nZCoiYYaxVG",
    "outputId": "e9b55901-8af9-4910-e946-0e4fe627450f"
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_dataset[60941]['image'].numpy().transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "colab_type": "code",
    "id": "dE2zB4_Vc5wW",
    "outputId": "438dab12-c5d0-415e-8385-0fccb94d1adf"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "data = next(dataiter)\n",
    "images, labels = data['image'], data['age']\n",
    "\n",
    "# Plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "plot_size = 20\n",
    "\n",
    "# Use integer division to ensure the number of columns is an integer\n",
    "for idx in np.arange(plot_size):\n",
    "    ax = fig.add_subplot(2, plot_size // 2, idx + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
    "    ax.set_title(str(labels[idx].item()))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xtz8TiR0HW8a"
   },
   "source": [
    "# Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5tjfSolfHf9n"
   },
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGR-ofHuEFf4"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G8vdR9BDFP0C"
   },
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def conv(in_channels, out_channels, kernel_size=4, stride=2, padding=1, batch_norm=True):\n",
    "    layers = []\n",
    "    conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "    layers.append(conv_layer)\n",
    "    \n",
    "    if batch_norm:\n",
    "        bn = nn.BatchNorm2d(out_channels)\n",
    "        layers.append(bn)\n",
    "        \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Gl3qi7eIgxG"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, y_size, conv_dim=64):\n",
    "        '''\n",
    "        Initialize the Discriminator Module\n",
    "        :param conv_dim: The depth of the first convolutional layer\n",
    "        :param y_size: The number of conditions \n",
    "        '''\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv_dim = conv_dim\n",
    "        self.y_size = y_size\n",
    "        self.conv1 = conv(3, conv_dim, 4, batch_norm=False)\n",
    "        self.conv2 = conv(conv_dim+y_size, conv_dim * 2, 4)\n",
    "        self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n",
    "        self.conv4 = conv(conv_dim*4, conv_dim*8, 4)\n",
    "        self.conv5 = conv(conv_dim*8, 1, 4, 1, 0, batch_norm=False)\n",
    "            \n",
    "    def forward(self, x, y):\n",
    "        '''\n",
    "        Forward propagation of the neural network\n",
    "        :param x: The input scaled image x\n",
    "        :param y: One-hot encoding condition tensor y (N,y_size)\n",
    "        :return: Discriminator logits; the output of the neural network\n",
    "        '''\n",
    "        x = F.relu(self.conv1(x))\n",
    "        y = y.view(-1,y.size()[-1],1,1)\n",
    "        y = y.expand(-1,-1,x.size()[-2], x.size()[-1])\n",
    "        x = torch.cat([x, y], 1)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.conv5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g0Q3nuJOHz1o"
   },
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BXL4DQo5IvJm"
   },
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def deconv(in_channels, out_channels, kernel_size=4, stride=2, padding=1, batch_norm=True):\n",
    "    \n",
    "    layers = []\n",
    "    t_conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "    layers.append(t_conv)\n",
    "    \n",
    "    if batch_norm:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gg_8hGGg9KJL"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_size, y_size, conv_dim=64):\n",
    "        '''\n",
    "        Initialize the Generator Module\n",
    "        :param z_size: The length of the input latent vector, z\n",
    "        :param conv_dim: The depth of the inputs to the *last* transpose convolutional layer\n",
    "        '''\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.conv_dim = conv_dim\n",
    "        \n",
    "        # self.fc = nn.Linear(z_size+y_size, conv_dim*8*4*4)\n",
    "        self.t_conv1 = deconv(z_size+y_size, conv_dim*8, 4, 1, 0)\n",
    "        self.t_conv2 = deconv(conv_dim*8, conv_dim*4, 4)\n",
    "        self.t_conv3 = deconv(conv_dim*4, conv_dim*2, 4)\n",
    "        self.t_conv4 = deconv(conv_dim*2, conv_dim, 4)\n",
    "        self.t_conv5 = deconv(conv_dim, 3, 4, batch_norm=False)\n",
    "        \n",
    "    def forward(self, z, y):\n",
    "        '''\n",
    "        Forward propagation of the neural network\n",
    "        :param x: The input to the neural network\n",
    "        :param y: The input condition to the neural network, Tensor (N,y_size)\n",
    "        :return: A 64x64x3 Tensor image as output\n",
    "        '''\n",
    "        x = torch.cat([z, y], dim=1)\n",
    "        x = x.view(-1, x.size()[-1], 1, 1)\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.relu(self.t_conv2(x))\n",
    "        x = F.relu(self.t_conv3(x))\n",
    "        x = F.relu(self.t_conv4(x))\n",
    "        x = self.t_conv5(x)\n",
    "        x = torch.tanh(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xGsty3wYIFRq"
   },
   "source": [
    "## Build complete network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 713
    },
    "colab_type": "code",
    "id": "fMZsaBy79lPw",
    "outputId": "efc45abd-1b6d-4529-a8c3-efe1ac5f58e4"
   },
   "outputs": [],
   "source": [
    "# define hyperparams\n",
    "conv_dim = 64\n",
    "z_size = 100\n",
    "y_size = 6 # no. of age classes\n",
    "\n",
    "# define discriminator and generator\n",
    "D = Discriminator(y_size, conv_dim)\n",
    "G = Generator(z_size, y_size, conv_dim)\n",
    "\n",
    "print(D)\n",
    "print()\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8oVuUkypIO7S"
   },
   "source": [
    "## Discriminator and Generator Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ScKFn1ZB9qbY",
    "outputId": "0a73b257-63cf-473d-d404-d40851c0ca17"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q7_sEm4fISzB"
   },
   "outputs": [],
   "source": [
    "def real_loss(D_out, smooth=False):\n",
    "    batch_size = D_out.size(0)\n",
    "    # label smoothing\n",
    "    if smooth:\n",
    "        # smooth, real labels = 0.9\n",
    "        labels = torch.ones(batch_size)*0.9\n",
    "    else:\n",
    "        labels = torch.ones(batch_size) # real labels = 1\n",
    "    # move labels to GPU if available     \n",
    "    labels = labels.to(device)\n",
    "    # binary cross entropy with logits loss\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss\n",
    "\n",
    "def fake_loss(D_out):\n",
    "    batch_size = D_out.size(0)\n",
    "    labels = torch.zeros(batch_size) # fake labels = 0\n",
    "    labels = labels.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hra3ltXxIYH7"
   },
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WFZqaTTsIVfq"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# params\n",
    "lr = 0.0002\n",
    "beta1=0.5\n",
    "beta2=0.999 # default value\n",
    "\n",
    "# Create optimizers for the discriminator and generator\n",
    "d_optimizer = optim.Adam(D.parameters(), lr, [beta1, beta2])\n",
    "g_optimizer = optim.Adam(G.parameters(), lr, [beta1, beta2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t4As0AzGIfWS"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GdRK5VL9VJFq"
   },
   "outputs": [],
   "source": [
    "def checkpoint(G, D, epoch, model, root_dir):\n",
    "    target_dir = f'{root_dir}/{model}'\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    G_path = os.path.join(target_dir, f'G_{epoch}.pkl')\n",
    "    D_path = os.path.join(target_dir, f'D_{epoch}.pkl')\n",
    "    torch.save(G.state_dict(), G_path)\n",
    "    torch.save(D.state_dict(), D_path)\n",
    "\n",
    "def oh_to_class(fixed_y):\n",
    "    age_map = {0:'0-18',1:'19-29',2:'30-39',3:'40-49',4:'50-59',5:'60+'}\n",
    "    if torch.cuda.is_available():\n",
    "        fixed_y = fixed_y.cpu()\n",
    "    fixed_y_idxs = fixed_y.numpy().nonzero()[1]\n",
    "    fixed_y_ages = [age_map[idx] for idx in fixed_y_idxs]\n",
    "    \n",
    "    return fixed_y_ages\n",
    "\n",
    "def save_samples_ages(samples, fixed_y, model, root_dir):\n",
    "    fixed_y_ages = oh_to_class(fixed_y)\n",
    "    samples_ages = {'samples': samples, 'ages': fixed_y_ages}\n",
    "    target_dir = f'{root_dir}/{model}'\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    with open(f'{target_dir}/train_samples_ages.pkl', 'wb') as f:\n",
    "        pkl.dump(samples_ages, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1703
    },
    "colab_type": "code",
    "id": "vra3o0xdIb5m",
    "outputId": "78c96424-4ab1-4fa8-ccf4-8a2ef2d2c9c5"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "root_dir = '/content/Age-cGAN'\n",
    "model = 'GAN_1'\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "# move models to GPU, if available\n",
    "G.to(device)\n",
    "D.to(device)\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "# training hyperparams\n",
    "num_epochs = 50\n",
    "\n",
    "# keep track of loss and generated, \"fake\" samples\n",
    "samples = []\n",
    "losses = []\n",
    "\n",
    "print_every = 300\n",
    "\n",
    "# Get some fixed data for sampling. These are images that are held\n",
    "# constant throughout training, and allow us to inspect the model's performance\n",
    "sample_size=16\n",
    "fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
    "fixed_z = torch.from_numpy(fixed_z).float()\n",
    "fixed_y = np.random.randint(len(bins), size=sample_size)\n",
    "fixed_y = fixed_y.reshape(-1,1)\n",
    "fixed_y = torch.zeros(sample_size, len(bins)+1).scatter_(1, torch.tensor(fixed_y), 1)\n",
    "\n",
    "# train the network\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for batch_i, batch in enumerate(train_loader):\n",
    "                \n",
    "        batch_size = batch['image'].size(0)\n",
    "        \n",
    "        # important rescaling image step\n",
    "        real_images = scale(batch['image'])\n",
    "        \n",
    "        # one-hot age\n",
    "        ages = one_hot(batch['age'], bins)\n",
    "        \n",
    "        # ============================================\n",
    "        #            TRAIN THE DISCRIMINATOR\n",
    "        # ============================================\n",
    "        \n",
    "        d_optimizer.zero_grad()\n",
    "        \n",
    "        # 1. Train with real images\n",
    "\n",
    "        \n",
    "        # Compute the discriminator losses on real images\n",
    "        real_images = real_images.to(device)\n",
    "        ages = ages.to(device)\n",
    "        \n",
    "        D_real = D(real_images, ages)\n",
    "        d_real_loss = real_loss(D_real)\n",
    "        \n",
    "        # 2. Train with fake images\n",
    "        \n",
    "        # Generate fake images\n",
    "        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "        z = torch.from_numpy(z).float()\n",
    "        # move x to GPU, if available\n",
    "        z = z.to(device)\n",
    "        #if train_on_gpu:\n",
    "        #    z = z.cuda()\n",
    "        fake_images = G(z, ages)\n",
    "        \n",
    "        # Compute the discriminator losses on fake images            \n",
    "        D_fake = D(fake_images, ages)\n",
    "        d_fake_loss = fake_loss(D_fake)\n",
    "        \n",
    "        # add up loss and perform backprop\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        \n",
    "        # =========================================\n",
    "        #            TRAIN THE GENERATOR\n",
    "        # =========================================\n",
    "        g_optimizer.zero_grad()\n",
    "        \n",
    "        # 1. Train with fake images and flipped labels\n",
    "        \n",
    "        # Generate fake images\n",
    "        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "        z = torch.from_numpy(z).float()\n",
    "        z = z.to(device)\n",
    "        fake_images = G(z, ages)\n",
    "        \n",
    "        # Compute the discriminator losses on fake images \n",
    "        # using flipped labels!\n",
    "        D_fake = D(fake_images, ages)\n",
    "        g_loss = real_loss(D_fake) # use real loss to flip labels\n",
    "        \n",
    "        # perform backprop\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Print some loss stats\n",
    "        if batch_i % print_every == 0:\n",
    "            # append discriminator loss and generator loss\n",
    "            losses.append((d_loss.item(), g_loss.item()))\n",
    "            # print discriminator and generator loss\n",
    "            print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
    "                    epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n",
    "\n",
    "    \n",
    "    ## AFTER EACH EPOCH##    \n",
    "    # generate and save sample, fake images\n",
    "    G.eval() # for generating samples\n",
    "    fixed_z = fixed_z.to(device)\n",
    "    fixed_y = fixed_y.to(device)\n",
    "    samples_z = G(fixed_z, fixed_y)\n",
    "    samples.append(samples_z)\n",
    "    G.train() # back to training mode\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint(G, D, epoch, model, root_dir)\n",
    "\n",
    "# Save training generator samples\n",
    "save_samples_ages(samples, fixed_y, model, root_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kAbLRxvOypob"
   },
   "source": [
    "## Training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "knGnWqnwymr8",
    "outputId": "924d40cc-e071-42ea-e9a2-2cca472496f1"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Discriminator', alpha=0.5)\n",
    "plt.plot(losses.T[1], label='Generator', alpha=0.5)\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lKEPOEOSzCNZ"
   },
   "source": [
    "## Generator samples from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iAqqI9ELzDxz"
   },
   "outputs": [],
   "source": [
    "# helper function for viewing a list of passed in sample images\n",
    "def view_samples(epoch, samples, ages):\n",
    "    fig, axes = plt.subplots(figsize=(16,4), nrows=2, ncols=8, sharey=True, sharex=True)\n",
    "    for ax, img, age in zip(axes.flatten(), samples[epoch], ages):\n",
    "        img = img.detach().cpu().numpy()\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        img = ((img +1)*255 / (2)).astype(np.uint8) # rescale to pixel range (0-255)\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        ax.set_title(age)\n",
    "        im = ax.imshow(img.reshape((64,64,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "ipJGPtFd0Jg6",
    "outputId": "192faf48-528a-48ac-ddda-19e295f75df0"
   },
   "outputs": [],
   "source": [
    "fixed_y_ages = oh_to_class(fixed_y)\n",
    "_ = view_samples(-1, samples, fixed_y_ages)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Age-cGAN.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "aplt_duke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
